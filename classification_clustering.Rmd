---
title:depression_anxiety_data
output: html_document
---
# phase 1

### Problem statement
The dataset aims to identify and evaluate the severity of depression and anxiety among undergraduate students at the University of Lahore. 
The problem revolves around understanding the impact of depression and anxiety on students' studies and social behavior

### Data mining task
The task involves utilizing the dataset as a basis for evaluating different machine-learning methods and approaches, particularly for the classification of the severity of depression and anxiety. 

Additionally, the dataset is suitable for comparing different machine-learning classification approaches to identify effective strategies for addressing mental health issues among students.

### Goal
Our main goal in collecting “Depression and anxiety” dataset is to learn more about the number of college students facing difficulties that end them up in having particular disorders , in our case depression and anxiety . We gather information about their school year, depressiveness, anxiousness, and Sleepiness. This kind of data will help collages make generally better decisions about their academic plans to what is more suitable for the students ,and also help the student affairs deanery improve its services or even add more to fulfill what a college student needs. It could also help mental health hospitals in knowing the importance of how to raise awareness in college students about their mental health sanity which could decrease the risk of college students developing depression/anxiety.

Here are three specific goals we have for this dataset:
1. Classification Goal:

We want to group students into different categories based on their PHQ Score, GAD Score, and EPWORTH Score. By doing this, we can see patterns and trends among different groups. This will help us understand if these specific scores have to do with students developing depression/anxiety. With this information, we can develop strategies that could help ensure lower the risks of developing depression/anxiety.

2. Defect Prediction Goal:

We want to use statistical techniques to seek trends that can predict any early symptoms of depression/anxiety. This could include things like, depressiveness, anxiousness, or Sleepiness. By catching these problems early, we can fix them and make sure students get help and achieve guidance toward the right path. This analysis will help make students mental health more “considerable”.

3. Clustering Goal:

We're clustering students together based on their similarities to create distinct clusters. Various methods will be utilized to determine the number of clusters that best suit our dataset, followed by evaluation techniques to assess the effectiveness of these clusters in segmenting the data effectively based on their inherent features.


## Dataset Details:

**Source of the dataset:** Kaggle

**Dataset Link:** https://www.kaggle.com/datasets/shahzadahmad0402/depression-and-anxiety-data?resource=download

**Number of Attributes:** 19

**Number of Objects:** 783

### Table :

| **Attribute Name**  | **Description**             | **Data Type**       | **Possible Values**                               |
|---------------------|-----------------------------|---------------------|----------------------------------------------------|
|ID                   |student id                   |Nominal              |3 values                                            |
|school_year          |student current year         |Ordinal              |1-4                                                 |
|Age                  |student age                  |Numeric              |18-24                                               |
|Gender               |student gender               |Binary               |female/male                                         |
|BMI                  |body mass index              |Numerical            |9-50                                                |
|who_bmi              |bmi classification           |Nominal              |Underweight/Normal/Overweight/Obese                 |
|PHQ Score            |Patient Health Questionnaire |Ordinal              |0-27                                                |
|depression_severity  |severity of depression       |Nominal              |Mild/Moderate/Moderately Severe/Severe/None-Minimal |
|depressiveness       |depressiveness               |Binary               |TRUE/FALSE                                          |
|Suicidal             |likely to commit suicide     |Binary               |TRUE/FALSE                                          |  
|depression_diagnosis |depression diagnosis         |Binary               |TRUE/FALSE                                          |
|depression_treatment |treating depression          |Binary               |TRUE/FALSE                                          |
|GAD Score            |Generalized Anxiety Disorder |Ordinal              |0-<15                                               |
|anxiety_severity     |severity of anxiety          |Nominal              |Mild/Moderate/Moderately Severe/Severe/None-Minimal |
|anxiousness          |anxiousness                  |Binary               |TRUE/FALSE                                          |
|anxiety_diagnosis    |anxiety diagnosis            |Binary               |TRUE/FALSE                                          |
|anxiety_treatment    |treating of anxiety          |Binary               |TRUE/FALSE                                          |
|EPWORTH Score        |The Epworth Sleepiness Scale |ordinal              |0-15                                                |
|Sleepiness           |state of being sleepy        |Binary               |TRUE/FALSE                                          |


# phase 2
# Loading dataset

```{r}
#loading the neceassary libraries 
suppressWarnings({
  library(dplyr)
library(readr)
library(caret)
library(tidyverse)
library(rpart)
    library(rattle)
library(rpart.plot)
library(RColorBrewer)
    library(FSelector)



})
```

```{r}
dataset<-read_csv("depression_anxiety_data.csv")
```

```{r}
################################### preproccing steps

##find missing values
is.na(dataset)

```

```{r}
##count missing values
sum(is.na(dataset))

```

```{r}
#delete the missing values
dataset[dataset=="Not Availble"]<- NA
dim(dataset)
dataset = na.omit(dataset)
dim(dataset)
sum(is.na(dataset))
print(dataset)

```
i
```{r}
#Detect Outliers 
#install.packages("outliers")
library(outliers)

```

```{r}
Outage = outlier(dataset$age, logical =TRUE)
sum(Outage)
Find_outlier = which(Outage ==TRUE, arr.ind = TRUE)
Outage
Find_outlier

```

```{r}

#
Outbmi= outlier(dataset$bmi, logical =TRUE)
sum(Outbmi)
Find_outlier2 = which(Outbmi==TRUE, arr.ind = TRUE)
Outbmi
Find_outlier2

```

```{r}
#delet outliers
dataset= dataset[-Find_outlier,]
dataset= dataset[-Find_outlier2,]


```

```{r}
# data transformation (change the TRUR and False to  0 and 1 )


dataset$depressiveness = factor (dataset$depressiveness, levels = c(TRUE,FALSE), labels=c(1,0))
print(dataset)
dataset$suicidal = factor (dataset$suicidal, levels = c(TRUE,FALSE), labels=c(1,0))
print(dataset)
dataset$depression_diagnosis = factor (dataset$depression_diagnosis, levels = c(TRUE,FALSE), labels=c(1,0))
print(dataset)
dataset$depression_treatment = factor (dataset$depression_treatment, levels = c(TRUE,FALSE), labels=c(1,0))
print(dataset)
dataset$anxiousness = factor (dataset$anxiousness, levels = c(TRUE,FALSE), labels=c(1,0))
print(dataset)
dataset$anxiety_diagnosis = factor (dataset$anxiety_diagnosis, levels = c(TRUE,FALSE), labels=c(1,0))
print(dataset)
dataset$anxiety_treatment = factor (dataset$anxiety_treatment, levels = c(TRUE,FALSE), labels=c(1,0))
print(dataset)
dataset$sleepiness = factor (dataset$sleepiness, levels = c(TRUE,FALSE), labels=c(1,0))
print(dataset)



```

```{r}
#Discretization the values based on fixed set of predetermined criteria

#1- bmi discretizatio (indicate high body fatness)

breaks <- c(18,18.4,24.9,29.9,34.9,39.9,100)
dataset$bmi= cut(dataset$bmi, breaks = breaks)
print(dataset)

```

```{r}

#2- phq test score discretizatio(Depression Test Questionnaire)

breaks <- c(0,4,9,14,19,27)
dataset$phq_score= cut(dataset$phq_score, breaks = breaks)
print(dataset)
```

```{r}
#3- gad test score discretization(Generalised Anxiety Disorder Assessment)


breaks <- c(0,4,9,14,21)
dataset$gad_score= cut(dataset$gad_score, breaks = breaks)
print(dataset)

```

```{r}


#4- Epworth test score discretizatio (Sleepiness Scale)

breaks <- c(0,10,14,17,24)
dataset$epworth_score= cut(dataset$epworth_score, breaks = breaks)
print(dataset)


######### end of preproccing 
```

```{r}
########### diagram for the class label (depressivness)
boolean_data <- dataset$depressiveness
print(boolean_data)

```

```{r}
# Count the occurrences of TRUE and FALSE values
true_count <- sum(boolean_data==1)
print(true_count)
```

```{r}
false_count <- sum(boolean_data==0) 
print(false_count)
```

```{r}
# Values for the bar chart
bar_heights <- c(true_count, false_count)

# Names for the bars
bar_names <- c("TRUE", "FALSE")

# Create a bar chart
barplot(bar_heights, names.arg = bar_names, col = c("blue", "red"),
        xlab = "Boolean Values", ylab = "Count",
        main = "class label (depressiveness)")
```
# phase 3
# Building model 
```{r}
# explore the dataset 
str(dataset)
```

```{r}
glimpse(dataset)
```

# Check if the data is balanced or not 

```{r}
# Calculate class distribution
class_distribution <- dataset %>% 
  group_by(depressiveness) %>% 
  summarise(count = n()) %>% 
  mutate(percentage = count / sum(count) * 100)

# Print class distribution
print(class_distribution)
```

```{r}
barplot(prop.table(table(dataset$depressiveness)),
        col = rainbow(2),
        ylim = c(0, 0.7),
        main = "Class Distribution")
```

```{r}
# Based on the plot it clearly evident that 73% of the data in 0 class and the remaining 26% in another class.

# So big difference observed in the amount of data available. If we are making a model based on these a dataset accuracy predicting students not admitted will be higher compared to students who are admitted.
```

# we can observe that the dataset is imbalanced 

<!-- Data Partition
Lets partition the dataset into train dataset and test dataset based on set.seed -->

# Over Sampling to handle imbalanced 

```{r}
library(ROSE)
library(caTools)
```

```{r}
# there is still missing value 
dataset <- na.omit(dataset)
```

```{r}
set.seed(123)
split <- sample.split(dataset$depressiveness, SplitRatio = 0.8)
train <- subset(dataset, split == TRUE)
test <- subset(dataset, split == FALSE)
table(train$depressiveness)
```

```{r}

# Perform oversampling using ovun.sample
over <- ovun.sample(depressiveness ~ ., data = train, method = "over",N=772)$data
table(over$depressiveness)
actual_labels<-test$depressiveness
```

```{r}
barplot(prop.table(table(over$depressiveness)),
        col = rainbow(2),
        ylim = c(0, 0.7),
        main = "Class Distribution")
```

```{r}
# as we can see the data of training is almost balanced 
```

```{r}
# Perform oversampling using ovun.sample
under <- ovun.sample(depressiveness ~ ., data = train, method = "under",N=310)$data
table(under$depressiveness)
```

```{r}
barplot(prop.table(table(under$depressiveness)),
        col = rainbow(2),
        ylim = c(0, 0.7),
        main = "Class Distribution")
```

# we will use over sampling for this example 

# Steps involved in the K-fold Cross Validation in R: 
- Split the data set into K subsets randomly
- For each one of the developed subsets of data points
- treat that subset as the validation (test) set
- Use all the rest subsets for training purpose
- Training of the model and evaluate it on the validation set or test set
- Calculate prediction error
- Repeat the above step K times i.e., until the model is not trained and tested on all subsets
- Generate overall prediction error by taking the average of prediction errors in every case

# let's rename our new sampling dataset  

```{r}
# assign the dataset to over sample datasets
set.seed(123)
dataset<-over
```

```{r}
#  make sure that the outcome column is a factor or numeric.
# we need to make sure the class lable is factor 
str(dataset$depressiveness)
```

```{r}
# we can see the datatype is binary as true and false 
#convert it to factor
dataset$depressiveness <- as.factor(dataset$depressiveness)
```

# DT with k-validation 5 and attribute selection information gain, gain ratio , gini index

```{r}
#make data frame to track the process of the different tree 
# Create an empty dataframe
evaluation_df <- data.frame(tree = character(), accuracy = numeric(),Precision = numeric(),Recall = numeric(),F1 = numeric(), Atrribute=character(),cv_K=character(),stringsAsFactors = FALSE)
```

# Spilt the data into k volds 

```{r}
folds <- createFolds(dataset$depressiveness, k = 5)  # create the folds
# control parameters 
train_control <- trainControl(method = "cv", index = folds)
```

### information gain 

```{r}
#build the model
tree1 <- train(depressiveness ~ ., data = dataset, trControl = train_control, 
               method = "rpart", parms = list(split = "information"))
```

```{r}
# train result
trained_model <- tree1$finalModel
trained_model
```

```{r}
#features importance
trained_model$variable.importance
```

```{r}
#summary of the trainig 
summary(trained_model)
```

```{r}
#accuracy of the tree 
accuracy <- tree1$results$Accuracy
print(paste("Accuracy:", accuracy))
```

```{r}
# get the average accuracy across all fold
accuracy <- mean(tree1$results$Accuracy)
print(paste("Average Accuracy:", accuracy))
```

```{r}
predictions <- predict(tree1, test)
length(predictions)
```

```{r}
length(test$depressiveness)
```

```{r}
# predict on test data
predictions <- predict(tree1, newdata=test)
predictions <- factor(predictions, levels = levels(test$depressiveness))

# Proceed with creating the confusion matrix and calculating metrics
confusion <- confusionMatrix(predictions, test$depressiveness)

# Getting the accuracy from the confusion matrix
accuracy <- confusion$overall['Accuracy']

# Calculate precision
precision <- confusion$byClass["Precision"]

# Calculate recall (sensitivity)
recall <- confusion$byClass["Recall"]

# Calculate F1 score
f1_score <- confusion$byClass["F1"]

# Print the results
print(confusion)

# add to the dataset of evalution 
tree1 <- valuation_df <- data.frame(tree = "tree1", accuracy = accuracy,Precision =precision,Recall =recall ,F1=f1_score, Atrribute="information gain",
                                       cv_K=5)
evaluation_df <- rbind(evaluation_df, tree1)
# visulize the tree'
fancyRpartPlot(trained_model, caption = NULL)
```

## Gini index

```{r}
dataset<-over
folds <- createFolds(dataset$depressiveness, k = 5)  # create the folds
# control parameters 
train_control <- trainControl(method = "cv", index = folds)
```

```{r}
#build the model
tree <- train(depressiveness ~ ., data = dataset, trControl = train_control, 
               method = "rpart")
```

```{r}
# train result
trained_model <- tree$finalModel
trained_model
```

```{r}
trained_model$variable.importance
```

```{r}
#summary of the trainig 
summary(trained_model)
```

```{r}
#accuracy of the tree 
accuracy <- tree$results$Accuracy
print(paste("Accuracy:", accuracy))
# get the average accuracy across all fold
accuracy <- mean(tree$results$Accuracy)
print(paste("Average Accuracy:", accuracy))
```

```{r}
predict(tree, test)
```

```{r}
# predict on test data
predictions <- predict(tree, test)
predictions <- factor(predictions, levels = levels(test$depressiveness))

# Proceed with creating the confusion matrix and calculating metrics
confusion <- confusionMatrix(predictions, test$depressiveness)

# Getting the accuracy from the confusion matrix
accuracy <- confusion$overall['Accuracy']

# Calculate precision
precision <- confusion$byClass["Precision"]

# Calculate recall (sensitivity)
recall <- confusion$byClass["Recall"]

# Calculate F1 score
f1_score <- confusion$byClass["F1"]

# Print the results
print(confusion)

# add to the dataset of evalution 
tree1 <- valuation_df <- data.frame(tree = "tree2", accuracy = accuracy,Precision =precision,Recall =recall ,F1=f1_score, Atrribute="Ginin index",
                                       cv_K=5)
evaluation_df <- rbind(evaluation_df, tree1)
# visulize the tree'
fancyRpartPlot(trained_model, caption = NULL)
```

```{r}
evaluation_df
```

# Gain Ratio

```{r}
dataset<-over
folds <- createFolds(dataset$depressiveness, k = 5)  # create the folds
# control parameters 
train_control <- trainControl(method = "cv", index = folds)
```

```{r}
# Compute the gain ratio of attributes for predicting the depressiveness
weights <- gain.ratio(depressiveness ~ ., data = dataset)

# Print the weights
print(weights)

#build the model 
tree <- train(depressiveness ~ ., data = dataset, trControl = train_control, 
               method = "rpart", parms = list(split = weights))
```

```{r}

trained_model <- tree$finalModel
trained_model

trained_model$variable.importance

#accuracy of the tree 
accuracy <- tree$results$Accuracy
print(paste("Accuracy:", accuracy))

# get the average accuracy across all fold
accuracy <- mean(tree$results$Accuracy)
print(paste("Average Accuracy:", accuracy))
```

```{r}
# predict on test data
predictions <- predict(tree, test)
predictions <- factor(predictions, levels = levels(test$depressiveness))

# Proceed with creating the confusion matrix and calculating metrics
confusion <- confusionMatrix(predictions, test$depressiveness)

# Getting the accuracy from the confusion matrix
accuracy <- confusion$overall['Accuracy']

# Calculate precision
precision <- confusion$byClass["Precision"]

# Calculate recall (sensitivity)
recall <- confusion$byClass["Recall"]

# Calculate F1 score
f1_score <- confusion$byClass["F1"]

# Print the results
print(confusion)

# add to the dataset of evalution 
tree1 <- valuation_df <- data.frame(tree = "tree3", accuracy = accuracy,Precision =precision,Recall =recall ,F1=f1_score, Atrribute="Gain ratio",
                                       cv_K=5)
evaluation_df <- rbind(evaluation_df, tree1)
# visulize the tree'
fancyRpartPlot(trained_model, caption = NULL)
```

```{r}
evaluation_df
```

#  DT with k-validation 7 and attribute selection information gain, gain ratio , gini index

```{r}
dataset<-over
folds <- createFolds(dataset$depressiveness, k = 7)  # create the folds
# control parameters 
train_control <- trainControl(method = "cv", index = folds)
```

```{r}
train_control
```

# Information gain - k=7

```{r}
#build the model
tree <- train(depressiveness ~ ., data = dataset, trControl = train_control, 
               method = "rpart", parms = list(split = "information"))
```

```{r}
# train result
trained_model <- tree$finalModel
trained_model
#features importance
trained_model$variable.importance

#accuracy of the tree 
accuracy <- tree$results$Accuracy
print(paste("Accuracy:", accuracy))
# get the average accuracy across all fold
accuracy <- mean(tree$results$Accuracy)
print(paste("Average Accuracy:", accuracy))
```

```{r}
# predict on test data
predictions <- predict(tree, test)
predictions <- factor(predictions, levels = levels(test$depressiveness))

# Proceed with creating the confusion matrix and calculating metrics
confusion <- confusionMatrix(predictions, test$depressiveness)

# Getting the accuracy from the confusion matrix
accuracy <- confusion$overall['Accuracy']

# Calculate precision
precision <- confusion$byClass["Precision"]

# Calculate recall (sensitivity)
recall <- confusion$byClass["Recall"]

# Calculate F1 score
f1_score <- confusion$byClass["F1"]

# Print the results
print(confusion)

# add to the dataset of evalution 
tree1 <- valuation_df <- data.frame(tree = "tree4", accuracy = accuracy,Precision =precision,Recall =recall ,F1=f1_score, Atrribute="Information Gain",
                                       cv_K=7)
evaluation_df <- rbind(evaluation_df, tree1)
# visulize the tree'
fancyRpartPlot(trained_model, caption = NULL)
```

```{r}
evaluation_df
```

# Gini index - k=7

```{r}
dataset<-over
folds <- createFolds(dataset$depressiveness, k = 7)  # create the folds
# control parameters 
train_control <- trainControl(method = "cv", index = folds)
```

```{r}
#build the model
tree <- train(depressiveness ~ ., data = dataset, trControl = train_control, 
               method = "rpart", parms = list(split = "gini"))
```

```{r}
# train result
trained_model <- tree$finalModel
trained_model
#features importance
trained_model$variable.importance

#accuracy of the tree 
accuracy <- tree$results$Accuracy
print(paste("Accuracy:", accuracy))
# get the average accuracy across all fold
accuracy <- mean(tree$results$Accuracy)
print(paste("Average Accuracy:", accuracy))
```

```{r}
# predict on test data
predictions <- predict(tree, test)
predictions <- factor(predictions, levels = levels(test$depressiveness))

# Proceed with creating the confusion matrix and calculating metrics
confusion <- confusionMatrix(predictions, test$depressiveness)

# Getting the accuracy from the confusion matrix
accuracy <- confusion$overall['Accuracy']

# Calculate precision
precision <- confusion$byClass["Precision"]

# Calculate recall (sensitivity)
recall <- confusion$byClass["Recall"]

# Calculate F1 score
f1_score <- confusion$byClass["F1"]

# Print the results
print(confusion)

# add to the dataset of evalution 
tree1 <- valuation_df <- data.frame(tree = "tree5", accuracy = accuracy,Precision =precision,Recall =recall ,F1=f1_score, Atrribute="Gini index",
                                       cv_K=7)
evaluation_df <- rbind(evaluation_df, tree1)
# visulize the tree'
fancyRpartPlot(trained_model, caption = NULL)
```

# Gain Ratio - k=7

```{r}
dataset<-under
folds <- createFolds(dataset$depressiveness, k = 7)  # create the folds
# control parameters 
train_control <- trainControl(method = "cv", index = folds)
```

```{r}
# Compute the gain ratio of attributes for predicting the depressiveness
weights <- gain.ratio(depressiveness ~ ., data = dataset)

# Print the weights
print(weights)

#build the model 
tree <- train(depressiveness ~ ., data = dataset, trControl = train_control, 
               method = "rpart", parms = list(split = weights))
```

```{r}
#build the model
tree <- train(depressiveness ~ ., data = dataset, trControl = train_control, 
               method = "rpart", parms = list(split = weights))
```

```{r}
# train result
trained_model <- tree$finalModel
trained_model
#features importance
trained_model$variable.importance

#accuracy of the tree 
accuracy <- tree$results$Accuracy
print(paste("Accuracy:", accuracy))
# get the average accuracy across all fold
accuracy <- mean(tree$results$Accuracy)
print(paste("Average Accuracy:", accuracy))
```

```{r}
fancyRpartPlot(trained_model, caption = NULL)
```

```{r}
# predict on test data
predictions <- predict(tree, test)
predictions <- factor(predictions, levels = levels(test$depressiveness))

# Proceed with creating the confusion matrix and calculating metrics
confusion <- confusionMatrix(predictions, test$depressiveness)

# Getting the accuracy from the confusion matrix
accuracy <- confusion$overall['Accuracy']

# Calculate precision
precision <- confusion$byClass["Precision"]

# Calculate recall (sensitivity)
recall <- confusion$byClass["Recall"]

# Calculate F1 score
f1_score <- confusion$byClass["F1"]

# Print the results
print(confusion)

# add to the dataset of evalution 
tree1 <- valuation_df <- data.frame(tree = "tree6", accuracy = accuracy,Precision =precision,Recall =recall ,F1=f1_score, Atrribute="Gain ratio",
                                       cv_K=7)
evaluation_df <- rbind(evaluation_df, tree1)
# visulize the tree'
fancyRpartPlot(trained_model, caption = NULL)
```

# DT with k-validation 10 and attribute selection information gain, gain ratio , gini index

# information gain - k=10

```{r}
dataset<-under
folds <- createFolds(dataset$depressiveness, k = 10)  # create the folds

# control parameters 
train_control <- trainControl(method = "cv", index = folds)

#build the model
tree <- train(depressiveness ~ ., data = dataset, trControl = train_control, 
               method = "rpart", parms = list(split = "information"))

# train result
trained_model <- tree$finalModel
trained_model

#features importance
trained_model$variable.importance

#accuracy of the tree 
accuracy <- tree$results$Accuracy
print(paste("Accuracy:", accuracy))

# get the average accuracy across all fold
accuracy <- mean(tree$results$Accuracy)
print(paste("Average Accuracy:", accuracy))
```

```{r}
# predict on test data
predictions <- predict(tree, test)
predictions <- factor(predictions, levels = levels(test$depressiveness))

# Proceed with creating the confusion matrix and calculating metrics
confusion <- confusionMatrix(predictions, test$depressiveness)

# Getting the accuracy from the confusion matrix
accuracy <- confusion$overall['Accuracy']

# Calculate precision
precision <- confusion$byClass["Precision"]

# Calculate recall (sensitivity)
recall <- confusion$byClass["Recall"]

# Calculate F1 score
f1_score <- confusion$byClass["F1"]

# Print the results
print(confusion)

# add to the dataset of evalution 
tree1 <- valuation_df <- data.frame(tree = "tree7", accuracy = accuracy,Precision =precision,Recall =recall ,F1=f1_score, Atrribute=" information Gain ",
                                       cv_K=10)
evaluation_df <- rbind(evaluation_df, tree1)
# visulize the tree'
fancyRpartPlot(trained_model, caption = NULL)
```

# Gini index - k=10

```{r}
dataset<-under
folds <- createFolds(dataset$depressiveness, k = 10)  # create the folds

# control parameters 
train_control <- trainControl(method = "cv", index = folds)

#build the model
tree <- train(depressiveness ~ ., data = dataset, trControl = train_control, 
               method = "rpart", parms = list(split = "gini"))

# train result
trained_model <- tree$finalModel
trained_model

#features importance
trained_model$variable.importance

#accuracy of the tree 
accuracy <- tree$results$Accuracy
print(paste("Accuracy:", accuracy))

# get the average accuracy across all fold
accuracy <- mean(tree$results$Accuracy)
print(paste("Average Accuracy:", accuracy))
```

```{r}
# predict on test data
predictions <- predict(tree, test)
predictions <- factor(predictions, levels = levels(test$depressiveness))

# Proceed with creating the confusion matrix and calculating metrics
confusion <- confusionMatrix(predictions, test$depressiveness)

# Getting the accuracy from the confusion matrix
accuracy <- confusion$overall['Accuracy']
# Calculate precision
precision <- confusion$byClass["Precision"]

# Calculate recall (sensitivity)
recall <- confusion$byClass["Recall"]

# Calculate F1 score
f1_score <- confusion$byClass["F1"]

# Print the results
print(confusion)

# add to the dataset of evalution 
tree1 <- valuation_df <- data.frame(tree = "tree8", accuracy = accuracy,Precision =precision,Recall =recall ,F1=f1_score, Atrribute="Gini index",
                                       cv_K=10)
evaluation_df <- rbind(evaluation_df, tree1)
# visulize the tree'
fancyRpartPlot(trained_model, caption = NULL)
```

# Gain ratio - k=10

```{r}
dataset<-under
folds <- createFolds(dataset$depressiveness, k = 10)  # create the folds

# control parameters 
train_control <- trainControl(method = "cv", index = folds)

# Compute the gain ratio of attributes for predicting the depressiveness
weights <- gain.ratio(depressiveness ~ ., data = dataset)

# Print the weights
print(weights)

#build the model 
tree <- train(depressiveness ~ ., data = dataset, trControl = train_control, 
               method = "rpart", parms = list(split = weights))
```

```{r}
# train result
trained_model <- tree$finalModel
trained_model

#features importance
trained_model$variable.importance

#accuracy of the tree 
accuracy <- tree$results$Accuracy
print(paste("Accuracy:", accuracy))

# get the average accuracy across all fold
accuracy <- mean(tree$results$Accuracy)
print(paste("Average Accuracy:", accuracy))
```

```{r}
# predict on test data
predictions <- predict(tree, test)
predictions <- factor(predictions, levels = levels(test$depressiveness))

# Proceed with creating the confusion matrix and calculating metrics
confusion <- confusionMatrix(predictions, test$depressiveness)

# Calculate precision
precision <- confusion$byClass["Precision"]

# Getting the accuracy from the confusion matrix
accuracy <- confusion$overall['Accuracy']

# Calculate recall (sensitivity)
recall <- confusion$byClass["Recall"]

# Calculate F1 score
f1_score <- confusion$byClass["F1"]

# Print the results
print(confusion)

# add to the dataset of evalution 
tree1 <- valuation_df <- data.frame(tree = "tree9", accuracy = accuracy,Precision =precision,Recall =recall ,F1=f1_score, Atrribute="Gain ratio",
                                       cv_K=10)
evaluation_df <- rbind(evaluation_df, tree1)
# visulize the tree'
fancyRpartPlot(trained_model, caption = NULL)
```

```{r}
evaluation_df
```


This table displays the results of classification using various methods for each K.


information gain :
+----------------------------------------+-------------------+----------------+-----------------------------+
|                                        | K=5               | K=7            | K=10                        |
+:======================================:+:=================:+:==============:+:===========================:+
| precision                              | 0.95              | 0.95           | 0.95                        |
+----------------------------------------+-------------------+----------------+-----------------------------+
| sensitivity                            | 1                 | 1              | 1                           |
+----------------------------------------+-------------------+----------------+-----------------------------+
| specificity                            | 1                  | 0.948          | 0.9487                     |
+----------------------------------------+-------------------+----------------+-----------------------------+
| Accuracy                               | 89.42%            |88.91%          | 88.59%                     |
+----------------------------------------+-------------------+----------------+-----------------------------+



Gain ratio:
+----------------------------------------+-------------------+----------------+-----------------------------+
|                                        | K=5               | K=7            | K=10                        |
+:======================================:+:=================:+:==============:+:===========================:+
| precision                              | 0.95              | 0.95           | 0.95                        |
+----------------------------------------+-------------------+----------------+-----------------------------+
| sensitivity                            | 1                 | 1              | 1                           |
+----------------------------------------+-------------------+----------------+-----------------------------+
| specificity                            | 1                  | 0.948         | 0.9487                     |
+----------------------------------------+-------------------+----------------+-----------------------------+
| Accuracy                               |90.85%             |88.11%          | 86.04%                     |
+----------------------------------------+-------------------+----------------+-----------------------------+

Gini index:
+----------------------------------------+-------------------+----------------+-----------------------------+
|                                        | K=5               | K=7            | K=10                        |
+:======================================:+:=================:+:==============:+:===========================:+
| precision                              | 0.95              | 0.95           | 0.95                        |
+----------------------------------------+-------------------+----------------+-----------------------------+
| sensitivity                            | 1                 | 1              | 1                           |
+----------------------------------------+-------------------+----------------+-----------------------------+
| specificity                            | 1                  | 0.948         | 0.9487                     |
+----------------------------------------+-------------------+----------------+-----------------------------+
| Accuracy                               |90.95%             | 89.47%         | 87.37%                     |
+----------------------------------------+-------------------+----------------+-----------------------------+








######## Clustering  ########

```{r}
# include the libraries and import the dataset 
library('superml')
library(cluster)
library(factoextra)
```

```{r}
# import the dataset 
df<-dataset
```

```{r}
head(df,10)
```

```{r}
str(df)
```

# let's prepocess the data before applay clustering as well as kmeans deal only with numeric we need to convert factor and chr to numeric value, like labels encoding

```{r}
# using label encoder
encoding <- LabelEncoder$new()
```

```{r}
# Identify categorical columns
categorical_cols <- dataset %>%
  select_if(is.character) %>%
  names()
```

```{r}
categorical_cols
```

```{r}
# Convert categorical columns to factors
df[categorical_cols] <- lapply(df[categorical_cols], as.factor)
```

```{r}
str(df)
```

```{r}
categorical_cols <- dataset %>%
  select_if(is.factor) %>%
  names()
categorical_cols
```

```{r}
for (col in categorical_cols)
{
    # Fit the LabelEncoder object to the data frame column
encoding$fit(df[[col]])

# Transform the data frame column using the LabelEncoder object
encoded_col <- encoding$fit_transform(df[[col]])
# Assign the encoded column to the data frame
df[[col]] <- encoded_col
}
```

```{r}
head(df,10)
```

```{r}
# still some data has not changed lets convert them manually 
encoding$fit(df$gender)

# Transform the data frame column using the LabelEncoder object
encoded_col <- encoding$fit_transform(df$gender)

# Assign the encoded column to the data frame
df$gender <- encoded_col
```

```{r}
# still some data has not changed lets convert them manually 
encoding$fit(df$depression_severity)

# Transform the data frame column using the LabelEncoder object
encoded_col <- encoding$fit_transform(df$depression_severity)

# Assign the encoded column to the data frame
df$depression_severity <- encoded_col


# still some data has not changed lets convert them manually 
encoding$fit(df$who_bmi)

# Transform the data frame column using the LabelEncoder object
encoded_col <- encoding$fit_transform(df$who_bmi)

# Assign the encoded column to the data frame
df$who_bmi <- encoded_col


# still some data has not changed lets convert them manually 
encoding$fit(df$anxiety_severity)

# Transform the data frame column using the LabelEncoder object
encoded_col <- encoding$fit_transform(df$anxiety_severity)

# Assign the encoded column to the data frame
df$anxiety_severity <- encoded_col
```

```{r}
head(df)
```

```{r}
# lets spilt the data to target will not be used for clustering , also id not relvant to the case 
target <- data.frame(depressiveness = df$depressiveness)
predictors <- df[, -c(1,9)]
head(predictors)
```

# Define a function to compute BCubed precision and recall

```{r}

bcubed <- function(cluster, category) {
  # Check the input arguments
  if (length(cluster) != length(category)) {
    stop("cluster and category must have the same length")
  }
  if (any(is.na(cluster)) || any(is.na(category))) {
    stop("cluster and category must not contain NA values")
  }
  # Convert cluster and category to factors
  cluster <- factor(cluster)
  category <- factor(category)
  # Initialize the precision and recall vectors
  precision <- numeric(length(cluster))
  recall <- numeric(length(cluster))
  # Loop over each item
  for (i in 1:length(cluster)) {
    # Find the items in the same cluster as the current item
    same_cluster <- which(cluster == cluster[i])
    # Find the items in the same category as the current item
    same_category <- which(category == category[i])
    # Compute the precision and recall for the current item
    precision[i] <- length(intersect(same_cluster, same_category)) / length(same_cluster)
    recall[i] <- length(intersect(same_cluster, same_category)) / length(same_category)
  }
  # Return the average precision and recall
  return(c(precision = mean(precision), recall = mean(recall)))
}
```

##### determining k

##### Elbow method

This method determines the number of clusters according to the turning point in a curve, the curve is plotted using the total within-cluster sum of square (WSS) as in y-axis , and No. clusters in x-axis

```{r}

fviz_nbclust(df, kmeans, method = "wss") +
  geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")

```

k=4 
##### Silhouette method

Now we will apply Silhouette method to find the optimal number of clusters k, we will also plot a graph where x-axis represent the number of clusters and y-axis represent the average Silhouette coefficient

```{r}
fviz_nbclust(df, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")
```

k=2

As shown, the number of clusters k that represents the turning point in the curve is 4, so we will use it for clustering.

Lastly, we will use k=3 since it acheives the second highest average Silhouette coefficient, and since it's in the middle between 2 and 4 it will strike a balance between having too few clusters (k=2), and having several clusters (k=4), Thus, this choice will have an acceptable acuuracy.

# Cluster with k=4

```{r}
#perform k-means clustering with k = 4 clusters
km <- kmeans(predictors, centers = 4, nstart = 25)

#view results
km
```

```{r}
#plot results of k-means 
fviz_cluster(km, data =predictors )
```

```{r}
#decrase the cluster by 1 cause on label encoding we start with 0
nc<-km$cluster-1
```

```{r}
# bcubed precision and recall 
bcubed(nc, df$depressiveness)
```

```{r}
#Within-cluster sum of squares wss 
wss <- km$tot.withinss
print(wss)
```

```{r}

#average silhouette for each clusters
avg_sil <- silhouette(km$cluster,dist(df)) 
fviz_silhouette(avg_sil)
```

we can conclude from the graph and the results where k=4 is that the performance is worse than k=2 and k=3, because there is a noticeable overlapping between clusters. Also, the clusers' space is pretty wide which results in a large distance between objects in the same cluster. In addition, the recall is relatively low which might be a result of the overlapping and large distances between data objects. Furthermore, the Precision is high . We can also note that the WSS is the lowest indicating a lower compactness of clusters. Lastly, the average silhouette width is 0.03which is low reflecting high inter-cluster similarity.

# Cluster with k=3

```{r}
#perform k-means clustering with k = 3 clusters
km <- kmeans(predictors, centers = 3, nstart = 25)

#view results
km
```

```{r}
#plot results of k-means 
fviz_cluster(km, data = predictors)
```

```{r}
#decrase the cluster by 1 cause on label encoding we start with 0
nc<-km$cluster-1
```

```{r}
# bcubed precision and recall 
bcubed(nc, df$depressiveness)
```

```{r}
#Within-cluster sum of squares wss 
wss <- km$tot.withinss
print(wss)
```

```{r}

#average silhouette for each clusters
avg_sil <- silhouette(km$cluster,dist(df)) 
fviz_silhouette(avg_sil)
```

we can conclude from the graph and the results where k=3 is that the performance worse than k=2, because there is overlapping between clusters. In addition, the recall is relatively low , However, the Precision is lower thn k=4 . We can also note that the WSS indicates an intermidiate compactness of clusters, and that objects in a cluster are to some extent similar to one another. Lastly, the average silhouette width is 0.13 which reflects high inter-cluster similarity.

# Cluster with k=2

```{r}
#perform k-means clustering with k = 2 clusters
km <- kmeans(predictors, centers = 2, nstart = 25)

#view results
km
```

```{r}
#plot results of k-means 
fviz_cluster(km, data = predictors)
```

```{r}
#decrase the cluster by 1 cause on label encoding we start with 0
nc<-km$cluster-1
```

```{r}
# bcubed precision and recall 
bcubed(nc, df$depressiveness)
```

```{r}
#Within-cluster sum of squares wss 
wss <- km$tot.withinss
print(wss)
```

```{r}

#average silhouette for each clusters
avg_sil <- silhouette(km$cluster,dist(df)) 
fviz_silhouette(avg_sil)
```

we can conclude from the graph and the results that the k=2 is the optimal k, since there is less overlapping between the two clusters. Also, the recall is relatively high and is the highest among the k's chosen, the Precision is low (0.28) which could be duo to presence of outliers or sensitivity to Initial Centroid. We can also note that the WSS is 5680, indicating a good compactness of clusters, and that objects in a cluster are similar to one another noting that the higher the k, the lower the WSS. Lastly, the average silhouette width is 0.46 which is considered high reflecting high intra-cluster similarity.

## Clustering results

This table displays the results of clustering using various methods for each K.

|                                        |    K=2     |    K=3     |    K=4     | 
|:-------------:|:-------------:|:-------------:|:-------------:|----|
|      **Average Silhouette width**      |    0.46    |    0.13    |    0.03    | 
| **total within-cluster sum of square** |  5680.407  |  4825.322  |  4247.212  | 
|          **BCubed precision**          | 0.5940060  | 0.6421454  | 0.7039766  | 
|           **BCubed recall**            | 0.5193949  | 0.3959195  | 0.3276772  | 
|               **visual**               | ![](2.png) | ![](3.png) | ![](4.png) | 

Based on these metrics, we can assess the performance of each K value:

-   Average Silhouette Width: The highest value is achieved at K = 2 (0.46), indicating that the clusters are relatively well-separated (not sparse) compared to the other K values.

-   Total Within-Cluster Sum of Squares: The lowest value is observed at K = 4 (4247.212), indicating better cluster compactness compared to K= 2 and K = 3.

-   BCubed Precision and Recall: precision highest at K = 4, recall highest at k=2 suggesting a better match between the clustering assignments and the ground truth or desired clustering structure.

## Comparison and Conclusion:


Clustering:
Clustering with K=2 has the highest silhouette width, indicating well-separated clusters.
As K increases, the silhouette width decreases, and the within-cluster sum of square decreases, suggesting more compact clusters.
BCubed precision and recall show varying trends with K, indicating trade-offs between precision and recall.

Classification:
The classification model shows consistent precision, sensitivity, and specificity across different values of K.
Accuracy decreases as K increases, indicating better performance with fewer clusters.




In this study, the classification algorithms consistently exhibit superior performance in accurately predicting outcomes based on the provided features. 
While clustering techniques may uncover inherent patterns and groupings within the data, their effectiveness in predicting specific classes is not as pronounced as observed in the classification results. 
Therefore, considering the metrics such as information gain, which include precision, sensitivity, specificity, and accuracy, it becomes evident that classification is the more suitable approach for this dataset and problem.
 The classification algorithms consistently achieve high precision, sensitivity, specificity, and accuracy across different values of K, outperforming the clustering algorithms evaluated with metrics such as silhouette width, within-cluster sum of squares, BCubed precision, and BCubed recall.






## Findings
Discussing our results and findings:
We employed both supervised and unsupervised learning techniques for our data analysis, specifically focusing on classification and clustering. as we mentioned before, our dataset represents the collage students' data to predict the probability of a university student to develop depressiveness in order to make people have the proper preventive measures that help them to make their lives better.

Therefore, we applied the data mining tasks which are classification and clustering. In the realm of classification, we utilized a decision tree algorithm, which is a recursive method that generates a tree structure with leaf nodes representing final decisions.

Our model's primary task was to predict the class label for (depressiveness), with two possible categories: TRUE and FALSE. This prediction is made on the rest attributes , including age, gender, BMI, WHO BMI, PHQ score, depression severity, suicidal, depression diagnosis, depression treatment, GAD score, anxiety severity, anxiousness, anxiety diagnosis, anxiety treatment, Empworth score, and sleepiness. In order to split our dataset into two distinct sets: the Training set and the Testing set. we used the Cross validation method and tried 3 different sizes (10,7,5) and applied 3 attribute selection method for each size (gain ratio, gini index, information gain) which results in 9 trees. First tree, resulting from k=5 and information gain, accuracy=89.42% Second tree, resulting from k=5 and gini index, accuracy=90.85% Third tree, resulting from k=5 and Gain ratio, accuracy=90.95% Fourth tree, resulting from k=7 and information gain, accuracy=88.91% Fifth tree, resulting from k=7 and gini index, accuracy=88.11% Sixth tree, resulting from k=7 and gain ratio, accuracy=89.47% Seventh tree, resulting from k=10 and information gain, accuracy=88.59% eighth tree, resulting from k=10 and gini index, accuracy=86.04% Ninth tree, resulting from k=10 and gain ratio, accuracy=87.37%

After calculating the average accuracy for each tree, we found that the third model has the best accuracy which means that most tuples were correctly classified. In the case of clustering, as it is unsupervised learning, we did not use a class label for implementing the cluster. We removed the class label attribute "depressiveness" and used all other attributes in clustering (age, gender, BMI, who_bmo, PHQ score, depression_severity, suicidal, depression_diagnosis, depression_treatment, GAD Score, anxiety_severity, anxiousness, anxiety_diagnosis, anxiety_treatment, EPWORTH Score, and sleepiness), all of which are different data types. We implemented the K-mean algorithm for clustering, which produces K clusters, with each cluster represented by the center point and each object assigned to the nearest cluster.

We iteratively recalculated the center and reassigned objects until the center point of each cluster did not change. To implement the clustering technique, We encoded the categorical values we want to use in clustering into numerical values, and removed the attributes that are irrelevant like student id. then we defined functions to calculate the average BCubed precision and recall. we used the elbow method to determine the optimal number of clusters, which gave us a result of k=5. we will experiment clustering with k=5, k=4 ,k=3. From the cluster plot we see that in k=5,4,3 clusters are close and overlapping (there are a lot of similarities between the data objects) The silhouette coefficient measured between all data are very small and some are even negative in all k=5,4,3. The average precision and recall found show that As the number of clusters decrease the precision decreases and recall increases.

We also calculated the difference between the actual results and our clustering results ,k=3 had a higher difference than k=4. And k=5 had a higher difference than k=3 . k=4 had the lowest difference thus is a better option than the other models. At the end, we evaluated our models by measuring the average accuracy to choose the most effective model.


## Refrences
[1]K. Mazidi, “RPubs - Data Mining: Classification with Decision Trees,” RPubs, 2016.  [Online]. Available: https://rpubs.com/kjmazidi/195428. [Accessed: Dec. 02, 2023]
[2]C. Guild, “RPubs - Classification and Regression Trees (CART) in R,” RPubs, 2021.  [Online]. Available: https://rpubs.com/camguild/803096. [Accessed: Dec. 02, 2023]
[3]S. BJUT, “Depression and anxiety data,” Kaggle, Jul. 29, 2022.  [Online]. Available: https://www.kaggle.com/datasets/shahzadahmad0402/depression-and-anxiety-data. [Accessed: Dec. 02, 2023]
